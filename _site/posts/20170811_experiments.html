<link rel="stylesheet" href="../css/weeklyUpdates.css">

<title>20170811 Experiments</title>

<h2 id="20170811">2017-08-11</h2>

<h2 id="experiments">EXPERIMENTS</h2>

<h3 id="lstmmodels">LSTM MODELS</h3>

<h4 id="1lstmmodelwithhiddendim256depth2">1. LSTM Model with hiddenDim=256, depth=2</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 91.61%, validation accuracy - 84.82%</p></li>

<li><p>Speaker-independent validation accuracy - 61.73%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 1: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=2</li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 2: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=2</li>
</ul>

<h4 id="2lstmmodelwithhiddendim256depth3">2. LSTM Model with hiddenDim=256, depth=3</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 90.03%, validation accuracy - 84.85%</p></li>

<li><p>Speaker-independent validation accuracy - 61.17%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth3-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 3: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=3</li>
</ul>

<p><img src="20170811/LSTM-h256-depth3-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 4: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=3</li>
</ul>

<h4 id="3lstmmodelwithhiddendim256depth2encodedinto64dimensions">3. LSTM Model with hiddenDim=256, depth=2, encoded into 64 dimensions*</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 84.17%, validation accuracy - 83.04%</p></li>

<li><p>Speaker-independent validation accuracy - 62.02%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 5: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=2, and an encoding Dense layer of dim=64</li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 6: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=2, and an encoding Dense layer of dim=64</li>
</ul>

<p><em>*</em>*Only ran for 100 epochs, can proceed further!!</p>

<h4 id="4seq2seqlstmmodelwithhiddendim256depth2similartowhatwewereusingbefore">4. Seq2Seq LSTM Model with hiddenDim=256, depth=2 (Similar to what we were using before)</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 90.10%, validation accuracy - 78.90%</p></li>

<li><p>Speaker-independent validation accuracy - 58.19%</p></li>
</ul>

<p><img src="20170811/LSTMSeq2Seq-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 7: Training accuracy, Validation accuracy vs Epoch, for model with Seq2Seq model of hiddenDim=256, depth=2</li>
</ul>

<p><img src="20170811/LSTMSeq2Seq-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 8: Training loss, Validation loss vs Epoch, for model with Seq2Seq model of hiddenDim=256, depth=2</li>
</ul>

<h3 id="critic-2">CRITIC</h3>

<h4 id="1criticusingpredictedwordfromlstmlipreader">1. Critic using predicted word from LSTMLipReader</h4>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-word-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 9: Training accuracy, Validation accuracy vs Epoch, for Critic using predicted word from LSTMLipReader</li>
</ul>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-word-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 10: Training loss, Validation loss vs Epoch, for Critic using predicted word from LSTMLipReader</li>
</ul>

<h4 id="2criticusing64dimensionalencodedvaluefromlstmlipreader">2. Critic using 64-dimensional encoded value from LSTMLipReader</h4>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-enc64-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 11: Training accuracy, Validation accuracy vs Epoch, for Critic using 64-dimensional encoded value from LSTMLipReader</li>
</ul>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-enc64-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 12: Training loss, Validation loss vs Epoch, for Critic using 64-dimensional encoded value from LSTMLipReader</li>
</ul>

<h3 id="finetuninggridcorpustohindi">FINE-TUNING - GRIDcorpus to HINDI</h3>

<h4 id="1finetuninglstmmodelwithhiddendim256depth3tohindilstmlipreader">1. Fine-tuning LSTM Model with hiddenDim=256, depth=3 to Hindi-LSTMLipReader</h4>

<p><img src="20170811/Hindi-LSTM-h256-depth3-Adam-2e-04-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 13: Training accuracy, Validation accuracy vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<p><img src="20170811/Hindi-LSTM-h256-depth3-Adam-2e-04-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 14: Training loss, Validation loss vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<h4 id="2finetuninglstmmodelwithhiddendim256depth2encodedinto64dimensionstohindilstmlipreader">2. Fine-tuning LSTM Model with hiddenDim=256, depth=2, encoded into 64 dimensions to Hindi-LSTMLipReader</h4>

<p><img src="20170811/Hindi-LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-2e-04-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 15: Training accuracy, Validation accuracy vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<p><img src="20170811/Hindi-LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-2e-04-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 16: Training loss, Validation loss vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<hr />