<link rel="stylesheet" href="../css/weeklyUpdates.css">

<h2 id="20170805">2018-03-09</h2>

<hr />

<p><a href="20180223/Movie_Translation.png"><img src="20180223/Movie_Translation.png" alt="NOT FOUND" title="Frame"/></a></p>

<ul>
<li> Dataset [<a href="https://drive.google.com/open?id=1NGMqzicU0JsCsmErCtr4DrnaV4rKgyRU">structure</a>]: Collected videos of dialogues for English, Hindi, Telugu - 2-6 seconds per video, 150-200 videos per actor, 2-3 actors per language [<a href="https://drive.google.com/open?id=1_plJTx3GsW4sw7SHXTfjCcpX7GX2yTwN">txt</a>]</li>
</ul>

<hr />

<h3> Mahesh Babu </h3>

<ul>
<li> Training <a href="https://github.com/voletiv/DeepLearningImplementations">Pix2Pix</a> network according to <a href="http://ritheshkumar.com/obamanet/">ObamaNet</a></li>

<li> 11,150 frames in train, 1,396 frames in val, 1,396 in test
    <ul>
    <li>clipped video segments according to dialogue and (course) face visibility inspection</li>
    <li>extracted faces for each frame of dialogue clips</li>
    <li>extracted mouth landmarks using <a href="dlib.net">dlib</a></li>
    <li>blackened the expanded mouth region (width scaled up by 1.2, height scaled up by 1.8)</li>
    <li>drew outer and inner lip contours from the mouth landmarks using "cv2.drawContours"</li>
    </ul>

<p><a href="20180309/Mahesh_Babu_0000_frame_combined_000.png"><img src="20180309/Mahesh_Babu_0000_frame_combined_000.png" alt="NOT FOUND" title="Frame"/></a></p>
<p class='fig'>Figure 1: left - Target image, right - Source image</p>

<li> Noisy set - some frames are not of Mahesh Babu</li>

<li> Example of Pix2Pix result:

<p><a href="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_current_batch_validation.png"><img src="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_current_batch_validation.png" alt="NOT FOUND" title="Frame"/></a></p>
<p class='fig'>Figure 2: 1 & 2 - Source images to pix2pic, 3 & 4 - Generated images from 1 & 2 respectively</p>

<li> Training progress:</li>

<p><a href="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_training.gif"><img src="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_training.gif" alt="NOT FOUND" title="Frame"/></a></p>
<p class='fig'>Figure 3: Progress of pix2pix training</p>

<li> Testing (val set) progress:</li>

<p><a href="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_validation.gif"><img src="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_validation.gif" alt="NOT FOUND" title="Frame"/></a></p>
<p class='fig'>Figure 4: Progress of pix2pix on unseen images</p>

<li> Losses:</li>

<p><a href="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_losses.png"><img src="20180309/1520525495_Mahesh_Babu_black_mouth_polygons_losses.png" alt="NOT FOUND" title="Frame"/></a></p>
<p class='fig'>Figure 5: Losses</p>

<li>Idea: Train on LRW, fine-tune on Indian_Movie_Translation dataset</li>

</ul>

<hr />

<h4> Dataset Collection - automated pipeline: (put on hold) </h4>

<p><a href="20180223/Data_collection_pipeline.png"><img src="20180223/Data_collection_pipeline.png" alt="NOT FOUND" title="Frame"/></a></p>

<ul>
<li>Extract audio, isolate voice/speech using Audacity [<a href="http://www.audacityteam.org/">GitHub</a>]</li>
<li>Perform Voice Activity Detection using WebRTC [<a href="https://github.com/wiseman/py-webrtcvad">GitHub</a>]</li>
<li>Check for speaker through audio features, or facial features via [<a href="https://github.com/rcmalli/keras-vggface">VGG Face</a>]</li>
<li>Annotate dialogues in ITRANS (possible use of <a href="liv.ai">liv.ai</a>)</li>
</ul>




