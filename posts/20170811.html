<link rel="stylesheet" href="../css/weeklyUpdates.css">

<h2 id="20170811">2017-08-11</h2>

<ul>
<li>Accuracies in paper: <a href="https://arxiv.org/pdf/1601.08188.pdf">LIPREADING WITH LONG SHORT-TERM MEMORY</a>:


<ul>
<li>Speaker-dependent accuracy - 79.4%</li>

<li>Speaker-independent accuracy - 79.6%</li></ul>
</li>
</ul>

<h3 id="completed">COMPLETED</h3>

<ul>
<li>Re-trained LSTM models (instead of SimpleSeq2Seq by farizrahman4u)</li>
</ul>

<h4 id="1lstmmodels">1. LSTM MODELS</h4>

<ul>
<li><p>Tried:</p>

<ul>
<li>LSTM -> word prediction


<ul>
<li>depth = 2</li>

<li>depth = 3</li></ul>
</li>

<li>LSTM -> encoding into (64) dimensions -> word prediction</li>

<li>LSTM -> encoding -> decoding via LSTM -> word prediction (similar to SimpleSeq2Seq)</li></ul></li>

<li><p>Reached up to 90% training accuracy, up to 85% validation accuracy, and 62% speaker-independent accuracy</p></li>
</ul>

<h4 id="2critic">2. CRITIC</h4>

<ul>
<li><p>Trained Critic using:
1) predicted word,
2) 64-dim encoded value</p></li>

<li><p>Critic using predicted word from LSTMLipReader - each video was trained by giving as input:
1) correct words,
2) predicted words (80% accuracy),
3) wrong words</p></li>

<li><p>Critic using encoded value from LSTMLipReader - each video was trained by giving as input:
        1) predicted words (80% accuracy),
        2) wrong words</p></li>

<li><p>Using encoded values instead of predicted words does seem to offer some advantage</p></li>
</ul>

<h4 id="3finetuninglipreadertohindi">3. FINE-TUNING LipReader to HINDI</h4>

<ul>
<li><p>Retained LSTM weights, replaced the last layer of LipReader models with Dense layers to match the Hindi vocabulary considered</p></li>

<li><p>Tried with:
1) LSTM -> word prediction,
2) LSTM -> encoding into (64) dimensions -> word prediction</p></li>

<li><p>Need to find out if graphs are good for progress</p></li>
</ul>

<h2 id="experiments">EXPERIMENTS</h2>

<h3 id="lstmmodels">LSTM MODELS</h3>

<h4 id="1lstmmodelwithhiddendim256depth2">1. LSTM Model with hiddenDim=256, depth=2</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 91.61%, validation accuracy - 84.82%</p></li>

<li><p>Speaker-independent validation accuracy - 61.73%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 1: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=2</li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 2: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=2</li>
</ul>

<h4 id="2lstmmodelwithhiddendim256depth3">2. LSTM Model with hiddenDim=256, depth=3</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 90.03%, validation accuracy - 84.85%</p></li>

<li><p>Speaker-independent validation accuracy - 61.17%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth3-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 3: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=3</li>
</ul>

<p><img src="20170811/LSTM-h256-depth3-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 4: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=3</li>
</ul>

<h4 id="3lstmmodelwithhiddendim256depth2encodedinto64dimensions">3. LSTM Model with hiddenDim=256, depth=2, encoded into 64 dimensions*</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 84.17%, validation accuracy - 83.04%</p></li>

<li><p>Speaker-independent validation accuracy - 62.02%</p></li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 5: Training accuracy, Validation accuracy vs Epoch, for model with LSTM of hiddenDim=256, depth=2, and an encoding Dense layer of dim=64</li>
</ul>

<p><img src="20170811/LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 6: Training loss, Validation loss vs Epoch, for model with LSTM of hiddenDim=256, depth=2, and an encoding Dense layer of dim=64</li>
</ul>

<p><em>*</em>*Only ran for 100 epochs, can proceed further!!</p>

<h4 id="4seq2seqlstmmodelwithhiddendim256depth2similartowhatwewereusingbefore">4. Seq2Seq LSTM Model with hiddenDim=256, depth=2 (Similar to what we were using before)</h4>

<ul>
<li><p>Speaker-dependent training accuracy - 90.10%, validation accuracy - 78.90%</p></li>

<li><p>Speaker-independent validation accuracy - 58.19%</p></li>
</ul>

<p><img src="20170811/LSTMSeq2Seq-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 7: Training accuracy, Validation accuracy vs Epoch, for model with Seq2Seq model of hiddenDim=256, depth=2</li>
</ul>

<p><img src="20170811/LSTMSeq2Seq-h256-depth2-Adam-1e-03-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 8: Training loss, Validation loss vs Epoch, for model with Seq2Seq model of hiddenDim=256, depth=2</li>
</ul>

<h3 id="critic-2">CRITIC</h3>

<h4 id="1criticusingpredictedwordfromlstmlipreader">1. Critic using predicted word from LSTMLipReader</h4>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-word-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 9: Training accuracy, Validation accuracy vs Epoch, for Critic using predicted word from LSTMLipReader</li>
</ul>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-word-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 10: Training loss, Validation loss vs Epoch, for Critic using predicted word from LSTMLipReader</li>
</ul>

<h4 id="2criticusing64dimensionalencodedvaluefromlstmlipreader">2. Critic using 64-dimensional encoded value from LSTMLipReader</h4>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-enc64-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 11: Training accuracy, Validation accuracy vs Epoch, for Critic using 64-dimensional encoded value from LSTMLipReader</li>
</ul>

<p><img src="20170811/C3DCritic-l1f4-l2f4-l3f8-fc1n8-vid8-enc64-oHn16-Adam-1e-04-GRIDcorpus-s0107-s0909-tMouth-vMouth-NOmeanSub-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 12: Training loss, Validation loss vs Epoch, for Critic using 64-dimensional encoded value from LSTMLipReader</li>
</ul>

<h3 id="finetuninggridcorpustohindi">FINE-TUNING - GRIDcorpus to HINDI</h3>

<h4 id="1finetuninglstmmodelwithhiddendim256depth3tohindilstmlipreader">1. Fine-tuning LSTM Model with hiddenDim=256, depth=3 to Hindi-LSTMLipReader</h4>

<p><img src="20170811/Hindi-LSTM-h256-depth3-Adam-2e-04-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 13: Training accuracy, Validation accuracy vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<p><img src="20170811/Hindi-LSTM-h256-depth3-Adam-2e-04-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 14: Training loss, Validation loss vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<h4 id="2finetuninglstmmodelwithhiddendim256depth2encodedinto64dimensionstohindilstmlipreader">2. Fine-tuning LSTM Model with hiddenDim=256, depth=2, encoded into 64 dimensions to Hindi-LSTMLipReader</h4>

<p><img src="20170811/Hindi-LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-2e-04-plotAcc.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 15: Training accuracy, Validation accuracy vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<p><img src="20170811/Hindi-LSTM-h256-depth2-LSTMactivtanh-enc64-encodedActivsigmoid-Adam-2e-04-plotLosses.png" alt="alt text" title="Frame" /></p>

<ul>
<li>Figure 16: Training loss, Validation loss vs Epoch, for LSTMLipReader with hiddenDim=256, depth=3 fine-tuned to Hindi-LSTMLipReader</li>
</ul>

<h3 id="todo-3">TO DO</h3>

<ul>
<li><p>Applications of Critic: improving precision of lip reader, semi-supervised setting, to pin-point top out of top-5 predictions</p></li>

<li><p>Read papers on critic-based methods</p>

<ul>
<li>Dhanraj - Has my algorithm succeeded? ECCV 2012</li>

<li>Deric Hoyem - ICCV 2013</li></ul></li>

<li><p>Sanskrit phonemes for Indic language lip reading</p></li>
</ul>

<hr />
