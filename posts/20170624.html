<link rel="stylesheet" href="weeklyUpdates.css">

<h2 id="20170624">2017-06-24</h2>

<h3 id="completed-2">Completed</h3>

<ul>
<li><p>Read papers:</p>

<ul>
<li><a href="http://www.cv-foundation.org/openaccess/content_iccv_2015_workshops/w12/papers/Koller_Deep_Learning_of_ICCV_2015_paper.pdf">Deep Learning of Mouth Shapes for Sign Language</a></li>

<li>TIMIT Database - <a href="https://www.intechopen.com/books/speech-technologies/phoneme-recognition-on-the-timit-database">39 (folded) phones</a> (Table 3); http://laotzu.bit.uni-bonn.de/ipec_presentation/speaker0.pdf] + 1 garbage</li>

<li><a href="https://link.springer.com/content/pdf/10.1007%2Fs11042-012-1128-7.pdf">Clustering Persian viseme using phoneme subspace for developing visual speech application</a> (need IIIT server)</li>

<li><a href="http://delivery.acm.org/10.1145/60000/57170/p19-petajan.pdf?ip=14.139.82.6&amp;id=57170&amp;acc=ACTIVE%20SERVICE&amp;key=045416EF4DDA69D9%2E1E2B3508530718A8%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35&amp;CFID=941095463&amp;CFTOKEN=15453391&amp;__acm__=1498134384_9aaadfc1a2b78e0006e00d48dd5d00b9">AN IMPROVED AUTOMATIC LIPREADING SYSTEM TO ENHANCE SPEECH RECOGNITION - E. Petajan</a> (need IIIT server)</li>

<li><a href="https://arxiv.org/pdf/1705.02966.pdf">You said that? - Zisserman</a>, <a href="https://www.youtube.com/watch?v=lXhkxjSJ6p8&amp;feature=youtu.be">video</a></li>

<li><a href="https://arxiv.org/pdf/1409.4842.pdf">Going deeper with convolutions</a></li>

<li><a href="https://arxiv.org/pdf/1512.00567.pdf">Rethinking the Inception Architecture for Computer Vision</a></li>

<li><a href="https://github.com/fchollet/deep-learning-models">Keras implementation</a></li></ul></li>

<li><p>Found other datasets online</p>

<p><ul>
<li><a href="http://spandh.dcs.shef.ac.uk/gridcorpus/">GRIDcorpus</a></li></p>

<p><li><a href="http://www.ee.surrey.ac.uk/Projects/LILiR/datasets.html">LILiR (Language-Independent Lip Reading)</a></li></p>

<p><li><a href="https://www-i6.informatik.rwth-aachen.de/~forster/database-rwth-phoenix.php">RWTH-PHOENIXWeathercorpus</a></li></p>

<p><li>LipNet - "Lipreading datasets (AVICar, AVLetters, AVLetters2, BBC TV, CUAVE, OuluVS1, OuluVS2) are plentiful (Zhou et al., 2014; Chung &amp; Zisserman, 2016a), but most only contain single words or are too small. One exception is the GRID corpus (Cooke et al., 2006)"</li></ul>

<p></p></li>
</ul></p>

<h3 id="discussion-4">Discussion</h3>

<ul>
<li><p>"Deep Learning of Mouth Shapes for Sign Language" trained mouth shapes without the need of mouth features (mouth landmarks, SICAAM, etc.) and achieved better accuracy</p></li>

<li><p>"LipNet" said "Lipreading with LSTM" and "Lip Reading Sentences in the Wild" are end-to-end trainable</p></li>

<li><p>LipNet (Nando de Freitas) achieves better accuracy than both, without mouth features or visemes</p></li>

<li><p>Do we want to improve LipNet instead?</p></li>

<li><p>"You said that?" (Zisserman) does one-shot learning to generate lip movement on face image live</p></li>
</ul>

<h3 id="todo-5">TO DO</h3>

<ul>
<li><p>Size of dataset</p></li>

<li><p>Pose estimation</p></li>
</ul>

<hr />
